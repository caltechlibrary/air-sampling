{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0be524-ff2d-452d-8967-37f4437f6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "\n",
    "from matplotlib.dates import HourLocator, DateFormatter\n",
    "\n",
    "import glob\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3105547-1a18-4caa-9b48-8dd3da7fa10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4dd1be2-53c8-433a-883a-66b11bbdf9cc",
   "metadata": {},
   "source": [
    "### PHOENIX analysis\n",
    "last update: May 22, 2025\n",
    "\n",
    "This is supposed to work for __Bokeh__. \n",
    "\n",
    "Create a script that:\n",
    "\n",
    "    - reads the data \n",
    "    - cleans it up \n",
    "    - 10min running median\n",
    "    - flags for RH \n",
    "    - categorize data based on EPA limits\n",
    "    - plots PM2.5 and PM10 in bokeh\n",
    "\n",
    "### read the csv files that Nikos's code downloads\n",
    "- get dataframes for each csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769d880c-96ee-4068-9397-884fd1e37aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "df_info = pd.read_csv(\"PHOENIX_info.csv\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        df_name = os.path.splitext(filename)[0].replace(\"-\", \"_\").lower()  # sanitize and lowercase\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n",
    "        \n",
    "for _, row in df_info.iterrows():\n",
    "    phoenix_id = row['PHOENIX_ID']\n",
    "    \n",
    "    if pd.isna(phoenix_id):\n",
    "        continue  # skip rows with missing ID\n",
    "    \n",
    "    df_name = phoenix_id.replace(\"-\", \"_\").lower()\n",
    "\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "\n",
    "        # Check if \"device name\" in df matches phoenix_id\n",
    "        if 'device_name' in df.columns and phoenix_id in df['device_name'].values:\n",
    "            # Add all columns from row to each matching row in the df\n",
    "            for col in df_info.columns:\n",
    "                if col != 'PHOENIX_ID':\n",
    "                    df[col] = row[col]\n",
    "\n",
    "            globals()[df_name] = df  # reassign in case you're using the updated version later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810bcef-640c-4814-a2b3-437ca07846db",
   "metadata": {},
   "source": [
    "#### Now I want to clean up the data. Basically get rid of some columns and work only with the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0431055c-f1c8-4e00-a8e8-9ca8ac6f23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and load raw data ---\n",
    "columns_to_keep = [\n",
    "    'timestamp_local', 'met.rh', 'met.temp', \n",
    "    'pm1', 'pm25', 'pm10', 'device_name',\n",
    "    'DATETIME installed (mm/dd/yy hh:mm)', 'LAT', 'LON'\n",
    "]\n",
    "\n",
    "# Load all CSVs into globals\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        df_name = os.path.splitext(filename)[0].replace(\"-\", \"_\").lower()\n",
    "        globals()[df_name] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86bf7e-69f1-457d-9d63-d533c8fa7c38",
   "metadata": {},
   "source": [
    "#### Start from Datetime installed and let's create 10 min running median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b429731-9b51-4251-bbe5-9f700af227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup ---\n",
    "columns_to_keep = [\n",
    "    'timestamp_local', 'met.rh', 'met.temp', \n",
    "    'pm1', 'pm25', 'pm10', 'device_name',\n",
    "    'DATETIME installed (mm/dd/yy hh:mm)', 'LAT', 'LON'\n",
    "]\n",
    "\n",
    "pm_columns = ['pm1', 'pm25', 'pm10']\n",
    "\n",
    "# --- 2. Load raw CSVs into globals ---\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.lower().endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        df_name = os.path.splitext(filename)[0].replace(\"-\", \"_\").lower()\n",
    "        globals()[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "# --- 3. Clean, filter, and compute rolling medians ---\n",
    "dfs_clean = []\n",
    "dfs_rm = []\n",
    "titles = []\n",
    "\n",
    "for _, row in df_info.iterrows():\n",
    "    phoenix_id = row['PHOENIX_ID']\n",
    "    \n",
    "    if pd.isna(phoenix_id):\n",
    "        continue  # skip rows with missing ID\n",
    "\n",
    "    df_key = phoenix_id.replace(\"-\", \"_\").lower()\n",
    "    \n",
    "    if df_key in globals():\n",
    "        df_raw = globals()[df_key]\n",
    "\n",
    "        # Check device name match\n",
    "        if 'device_name' in df_raw.columns and phoenix_id in df_raw['device_name'].values:\n",
    "            # Attach metadata\n",
    "            for col in df_info.columns:\n",
    "                if col != 'PHOENIX_ID':\n",
    "                    df_raw[col] = row[col]\n",
    "\n",
    "            # Keep only relevant columns\n",
    "            df_clean = df_raw[[col for col in columns_to_keep if col in df_raw.columns]].copy()\n",
    "\n",
    "            # Parse datetime columns\n",
    "            if 'timestamp_local' in df_clean.columns:\n",
    "                df_clean['timestamp_local'] = pd.to_datetime(df_clean['timestamp_local'], errors='coerce')\n",
    "            if 'DATETIME installed (mm/dd/yy hh:mm)' in df_clean.columns:\n",
    "                df_clean['DATETIME installed (mm/dd/yy hh:mm)'] = pd.to_datetime(\n",
    "                    df_clean['DATETIME installed (mm/dd/yy hh:mm)'], errors='coerce'\n",
    "                )\n",
    "\n",
    "            # Drop pre-installation data\n",
    "            if all(c in df_clean.columns for c in ['timestamp_local', 'DATETIME installed (mm/dd/yy hh:mm)']):\n",
    "                install_time = df_clean['DATETIME installed (mm/dd/yy hh:mm)'].iloc[0]\n",
    "                df_clean = df_clean[df_clean['timestamp_local'] >= install_time]\n",
    "\n",
    "            # Save cleaned DataFrame\n",
    "            clean_name = f\"{df_key}_clean\"\n",
    "            globals()[clean_name] = df_clean\n",
    "\n",
    "            # If valid for plotting, store in list\n",
    "            if 'pm10' in df_clean.columns and 'timestamp_local' in df_clean.columns:\n",
    "                dfs_clean.append(df_clean)\n",
    "                titles.append(phoenix_id)\n",
    "\n",
    "            # --- Rolling median calculation ---\n",
    "            df_rm = df_clean.copy()\n",
    "\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df_rm['timestamp_local']):\n",
    "                df_rm['timestamp_local'] = pd.to_datetime(df_rm['timestamp_local'], errors='coerce')\n",
    "\n",
    "            df_rm = df_rm.set_index('timestamp_local').sort_index()\n",
    "\n",
    "            for col in pm_columns:\n",
    "                if col in df_rm.columns:\n",
    "                    df_rm[f\"{col}_rm\"] = df_rm[col].rolling(\"10min\").median()\n",
    "\n",
    "            df_rm.reset_index(inplace=True)\n",
    "\n",
    "            # Save rolling median DataFrame\n",
    "            rm_name = f\"{df_key}_rm\"\n",
    "            globals()[rm_name] = df_rm\n",
    "            dfs_rm.append(df_rm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e820280c-2a20-4d1a-9580-a76d2d4d7b42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Flag for high RH \n",
    "- Coleen noticed there were issues with high RH and huge spikes. \n",
    "- Paul: these need to be cleaned up before we can start sharing data \n",
    "- for now, until we get a better to do this let's flag for high RH (>80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a73d8d-67e4-40ae-a588-f8babbd6de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_flagged = []\n",
    "\n",
    "for df in dfs_rm:\n",
    "    df_flagged = df.copy()\n",
    "    df_flagged['rh_flag'] = (df_flagged['met.rh'] > 80).astype(bool)\n",
    "    dfs_flagged.append(df_flagged)\n",
    "\n",
    "dfs_no_rh = [df[df['rh_flag'] == False].copy() for df in dfs_flagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f32390-98c0-474d-b513-9d444b88a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs_no_rh)):\n",
    "    dfs_no_rh[i]['timestamp_local'] = pd.to_datetime(dfs_no_rh[i]['timestamp_local'])\n",
    "    dfs_no_rh[i] = dfs_no_rh[i].drop_duplicates(subset='timestamp_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e43e5-239e-4281-8e35-612e530559c9",
   "metadata": {},
   "source": [
    "### Get rid of unwanted columns for now\n",
    "- create two dfs, one for PM2.5 and one for PM10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76093750-77ea-4591-90d0-3f3f57b35bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_all = pd.concat(dfs_no_rh, ignore_index=True)\n",
    "df_all = df_all.set_index('timestamp_local')\n",
    "\n",
    "df_avg = (\n",
    "    df_all\n",
    "    .groupby('device_name')\n",
    "    .resample('10min')\n",
    "    .agg({\n",
    "        'pm25_rm': 'mean',\n",
    "        'pm10_rm': 'mean',\n",
    "        'LAT': 'first',\n",
    "        'LON': 'first'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "columns_to_drop = [\"pm10_rm\"]\n",
    "df_pm25 = df_avg.drop(columns=columns_to_drop)\n",
    "\n",
    "columns_to_drop = [\"pm25_rm\"]\n",
    "df_pm10 = df_avg.drop(columns=columns_to_drop)\n",
    "\n",
    "def categorize_pm10(val):\n",
    "    if val <= 54:\n",
    "        return 'Good'\n",
    "    elif val <= 154:\n",
    "        return 'Moderate'\n",
    "    elif val <= 254:\n",
    "        return 'Unhealthy for Sensitive Groups'\n",
    "    elif val <= 354:\n",
    "        return 'Unhealthy'\n",
    "    elif val <= 424:\n",
    "        return 'Very Unhealthy'\n",
    "    else:\n",
    "        return 'Hazardous'\n",
    "\n",
    "def categorize_pm25(val):\n",
    "    if val <= 9:\n",
    "        return 'Good'\n",
    "    elif val <= 35.4:\n",
    "        return 'Moderate'\n",
    "    elif val <= 55.4:\n",
    "        return 'Unhealthy for Sensitive Groups'\n",
    "    elif val <= 125.4:\n",
    "        return 'Unhealthy'\n",
    "    elif val <= 225.4:\n",
    "        return 'Very Unhealthy'\n",
    "    else:\n",
    "        return 'Hazardous'\n",
    "\n",
    "df_pm10['epa_category_10'] = df_pm10['pm10_rm'].apply(categorize_pm10)\n",
    "df_pm25['epa_category_25'] = df_pm25['pm25_rm'].apply(categorize_pm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ed780-f5ab-4542-97ed-2d5129292112",
   "metadata": {},
   "source": [
    "### We're going to create data for an ajax endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379c9945-e572-408d-a8b2-59731174b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyproj import Transformer\n",
    "\n",
    "\n",
    "transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "df_pm10['x'], df_pm10['y'] = transformer.transform(df_pm10['LON'].values, df_pm10['LAT'].values)\n",
    "df_pm25['x'], df_pm25['y'] = transformer.transform(df_pm25['LON'].values, df_pm25['LAT'].values)\n",
    "\n",
    "df_pm10['timestamp_str'] = df_pm10['timestamp_local'].dt.strftime('%Y-%m-%dT%H-%M-%S')\n",
    "df_pm25['timestamp_str'] = df_pm25['timestamp_local'].dt.strftime('%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "df_full = pd.concat([df_pm10, df_pm25], ignore_index=True)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"output_json\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def nonan(x):\n",
    "    return list(x.dropna())\n",
    "\n",
    "def unique(x):\n",
    "    return list(pd.unique(x.dropna()))\n",
    "\n",
    "grouped = df_full.groupby(\"timestamp_str\").agg({\n",
    "    'x': unique,\n",
    "    'y': unique,\n",
    "    'epa_category_10': nonan,\n",
    "    'epa_category_25': nonan\n",
    "})\n",
    "\n",
    "\n",
    "# Iterate over each group and save as a JSON file dropna(subset=['epa_category_10','epa_category_25'])\n",
    "for timestamp, group in grouped.iterrows():\n",
    "    json_data = {\n",
    "        \"x\": group[\"x\"],\n",
    "        \"y\": group[\"y\"],\n",
    "        \"epa_category_10\": group[\"epa_category_10\"],\n",
    "        \"epa_category_25\": group[\"epa_category_25\"],\n",
    "    }\n",
    "    # Define the filename using the timestamp\n",
    "    filename = f\"{timestamp}.json\"\n",
    "    # Write the JSON file\n",
    "    with open(os.path.join(output_dir, filename), \"w\") as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff584868-9965-4bf8-885c-fb1d4f557837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f94ce-e303-4166-be36-14f715b70499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d0ee8-c8e2-4f7b-8ef7-c0cf0a709e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
